# The Birds and the Bees

#### Sam Kim, Nick Merrill, Crystal Stowell

<div class='video'>
<iframe width="672" height="378" src="http://www.youtube.com/embed/0ohAn30qqAU?&vq=hd720" frameborder="0" allowfullscreen></iframe>
</div>

## Overview

We built an infrastructure for optimization problems with the right amount of abstraction that would allow us to easily build new algorithms and problems independently.  This way, we can swap in new algorithms or new problems without starting from scratch. Our main goal was to solve constraint programming problems, particularly the Nurse Scheduling Problem. We implemented 3 metaheuristic optimization algorithms: Cuckoo Search (CS), Particle Swarm Optimization (PSO), and a hybrid of the two algorithms. These algorithms can of course also solve more basic problems.  We included four mathematical optimization test functions and three simple real-world problems in addition to the nurse scheduling problem to demonstrate the flexibility of our design as well as analyze the performance of the 3 algorithms.

The first algorithm, called the Cuckoo Search, is a metaheuristic optimization algorithm based on the behavior of the cuckoo bird [1] . Some cuckoo bird species parasitically lay their eggs in the nest of another bird species, where the likelihood of the host bird raising the bird as its own depends on how well the cuckoo mimics the egg of the host species. In this algorithm, an initial set of nests, which represent the solutions, are randomly generated.  These solutions are then updated over multiple generations.  The process of updating an individual solution is as follows: a random nest is chosen, and a new solution is generated by random-walking from this previous solution.  This new solution can then replace a different randomly chosen solution if it has a fitness value better than the original.  After this possible replacement of a solution, all of the nests are ranked by fitness and the worst fraction of the nests is replaced with random solutions.  This combination of mechanisms allows the solutions to search locally and globally at the same time for the optimal solution.

The second algorithm, called the Particle Swarm Optimization, is a relatively popular algorithm introduced in 1995, and based on the social behavior of a swarm. Each particle, which represents a solution, has a velocity that allows the particle to move around the search space. Over each generation, the particle’s position is updated using its velocity, which is biased towards the particle’s own best position and the globally best position. This mechanism of swarm intelligence is expected to move the entire swarm towards the best solutions. However, this algorithm has been criticized for failing to find global minima/maxima and instead getting stuck inside local extremes.

The last algorithm is a hybrid of the above two algorithms that combines all the above mechanisms in each iteration. First, a new solution is generated using the random walk which may replace another solution. Next, every solution is updated using the associated velocities as described above. Finally, the worst fraction of nests is replaced with random solutions. Although this has been proposed and implemented in literature, testing and analysis of its performance has been inadequate [2]. We expect that this algorithm will combine all of the advantages of both algorithms and make up for any disadvantages.

In addition, we built a website as the graphical user interface for our project.  Originally a terminal UI, it was agreed upon that a website was both more interactive and more descriptive.  [Here](http://optimizer.nickmerrill.me) you can select a problem, read all about it, enter your desired inputs, and choose an algorithm with which to solve.

## Planning

Three weeks ago, we set out to write an optimization algorithm.  The goal was to be able to solve real-world problems as complex as the nurse scheduling problem and find an optimal solution.  That goal has been met.

To see how our project played out in terms of planning, check out our documentation for the [initial draft]({{STATIC_URL}}doc/draft_specification.pdf) and the [final draft]({{STATIC_URL}}doc/checkpoint2.pdf) of our technical specification. These documents have been annotated with comments on which features have been implemented. We can see that for the most part, we stayed very close to our original goals, which include optimization of an objective function using Cuckoo Search, providing a set of optimized solutions, and appropriate interfacing of the problem in order to change which optimization algorithm is used.

Some features that were not included are a visualization of the results due to time constraints as well as using Levy flights for the Cuckoo Search due to technical issues with incorporating separate libraries.

Six objective functions have been included: the Fence Problem, the Michalewicz Problem, the Box Problem, the Egg Holder Function, the Rastrigin Function, and the Nurse Scheduling Problem.  The first two problems are univariate, the next two are bivariate, and the last two are multivariate.

Although we did not choose to implement modifications of the Cuckoo Search (including quantum-inspired CS, multi-object CS), we did implement two additional optimization algorithms and were able to compare the performances. This includes the Particle Swarm Optimization (PSO) algorithm as well as a hybrid of the two.  A friendly graphical user interface has also been added as a cool extension to allow for better user interactivity and ease of use.  This can be found at [http://optimizer.nickmerrill.me](http://optimizer.nickmerrill.me).

## Design and Implementation

Code: ([Backend](https://github.com/NicholasMerrill/Optimizer) | [Frontend](https://github.com/NicholasMerrill/OptimizerFrontend))

Throughout the development of the project, the original design of the algorithms and solutions classes was altered to meet the needs of each class.  Specifically, changes to the abstraction of the classes were made.  However, this did not create any difficulties.

Since there are numerous parameters in each of the algorithms, parameter selection can significantly effect the algorithms' performance. For Cuckoo Search, we decided to follow many parameters used by the original author since these seemed to have the best results, confirmed by consequent studies [1]. This includes having 15 nests and setting p_a=0.25 (the percentage abandoned in each generation). There is a lot of ongoing research in the parameter selection for PSO, and for which situations certain parameters are appropriate. We decided to go with having 50 particles per generation, and setting w=0.7 (the weight particle's momentum, influenced by its previous velocity), and phi_p,phi_g=1.5, both of which are weights given to velocity influenced by the local and global best solutions [3][4].

The separation and abstraction of the problems versus the algorithms made it very easy to add new problems. The abstract OptimizationProblem takes care of a lot of details related to variable constraints and interfacing with the algorithms and front-end. Implementing sub-classes only involves specifying these constraints and the fitness functions for the algorithm, and print functions for the front-end. For example, implementing the new test functions such as [RastriginMinProb](https://github.com/NicholasMerrill/Optimizer/blob/master/src/problems/RastriginMinProb.java) or [EggholderFuncProb](https://github.com/NicholasMerrill/Optimizer/blob/master/src/problems/EggholderFuncProb.java) only took at most 10 minutes each to write.

Unfortunately, we did not get to implement fully everything we wanted. For example, although Cuckoo Search works well on all of the problems, PSO and the hybrid do not always reach the correct answer on problems with multivariable constraints (i.e. ax + by < c). While they do reach an answer, it is not accurate enough to say that these work without bugs.

**Contributions of Members**

Naturally, all three of us needed to collaborate initially to choose algorithms, problems, and our approach, including language, features, and abstraction details. The abstraction allowed us to work relatively independently on separate aspects of the project without constantly interfering with each other's parts. Nick designed a lot of the abstraction inside the Solution and SolutionSet classes while Crystal implemented the specific details of the respective sub-classes. Additionally, Crystal designed the implementations of several problems including the [Nurse Scheduling Problem](https://github.com/NicholasMerrill/Optimizer/blob/master/src/problems/NurseSchedProb.java) to make sure it was compatible with the other parts of our project. Sam was responsible for implementing, testing, and analyzing most the algorithms. Nick also contributed significantly to setting up and maintaining the front-end website and terminal user interface.

### Analysis

<h4>Experimental Procedures</h4>

Three mathematical functions commonly used in literature as benchmarks for optimization algorithms were chosen in order to analyze the performance of our three algorithms, including convergence rate and precision [5]. We used the Rastrigin Function, the Rosenbrock Function, and the Eggholder Function. Tests were run by first normalizing the number of iterations in each algorithm such that each algorithm would take approximately the same time for a given function, and then collecting data of the best fitness over the iterations. 200 data points were collected for each run, and 50 trials for each algorithm were averaged and analyzed.

**Rastrigin Function:** This function is a fairly difficult problem due to its large search space and its large number of local minima which are regularly distributed. The plot to the right shows the function when n=2. Data was collected for n=10 over 3 seconds. As we can see in the graph below, the hybrid algorithm is very quick to converge to a good solution, but over a longer period of time the cuckoo search is able to find a better solution. The PSO performed the worst. The function is given below:

<div class='equation'>
    <img src="{{STATIC_URL}}img/rastequation.png" />
    <img style='margin-left:40px;' src="{{STATIC_URL}}img/rastequation2.png" />
</div>

<div class="images2">
    <img src="{{STATIC_URL}}img/rastrigin.jpg" />
    <img src="{{STATIC_URL}}img/rastgraph.png" />
</div>
<div class="images1">
    <img src="{{STATIC_URL}}img/rasttable.png" />
</div>

**Rosenbrock’s Valley:** This function is a classic optimization problem, also known as the banana function or the second function of De Jong. We can see in the plot for the function in 2 dimensions that the global minimum lies inside a long, narrow, flat valley. Although finding the valley is trivial, convergence to the global optimum is very difficult to the large search space. In order to gather meaningful data, we restricted the number of dimensions (n) to 2, and the search space to [-104, 104]. The function is defined as below:

<div class='equation'>
    <img src="{{STATIC_URL}}img/rosenequation.png" />
</div>
<div class='equation'>
    <img src="{{STATIC_URL}}img/rosenequation2.png" width="400px" />
</div>

<div class="images2">
    <img src="{{STATIC_URL}}img/rosenbrock.png" />
    <img src="{{STATIC_URL}}img/rosengraph.png" />
</div>
<div class="images1">
    <img src="{{STATIC_URL}}img/rosentable.png" />
</div>

Note that the above data is plotted on a logarithmic scale, so CS performs very poorly in comparison to the other 2 algorithms, and levels off well before reaching the minimum value. This can likely be attributed to the random nature of CS’s local and global search which does not allow it to easily find better solutions inside the valley. PSO and the hybrid algorithm performed very similarly, with the PSO finding a slightly better solution towards the end. This further supports the conjecture that the random solutions and random walk do not contribute to finding the global minimum and that the swarm nature of the two algorithms allow them to move towards a better local minimum.

**Eggholder Function:** This test function also contains numerous local minima, although both algorithms fared fairly well. Because both algorithms converged so quickly, data was only collected over half a second, which may place a greater emphasis on the overhead of setting up the initial population. We can see in the plot below that PSO and the hybrid converge very quickly to the global minimum. While CS takes slightly longer, it does reach the global minimum as well over a greater number of iterations (not shown in the graph).

<div class="equation">
    <img src="{{STATIC_URL}}img/eggequation.png" /><br />
    Minimum at (x,y) = (512,404.2319)= -959.6407 for -512 ≤ x,y ≤ 512
</div>


<div class="images2">
    <img src="{{STATIC_URL}}img/eggholder.jpg" alt="Eggholder Function" />
    <img src="{{STATIC_URL}}img/egggraph.png" alt="Eggholder performance table" />
</div>
<div class="images1">
    <img src="{{STATIC_URL}}img/eggtable.png" alt="Eggholder performance graph" />
</div>

## Reflection

We are very happy with the success of our project. 

Throughout the past few weeks, there was some concern that the nurse scheduling problem might not be very generalizable.  But after lots of reworking of the problem, it can be customized in a myriad of ways, allowing it to be useful for more than just a hospital.  Additionally, we had trouble making this problem compatible with our algorithms due to the discrete nature of the solutions for the problem. This was resolved by still using continuous variables, but rounding when determining the fitness and constraints.

Had there been more time, we would have liked to add another complicated, real-world problem.  For example, the Travelling Salesman Problem is a very well-known and interesting problem, so we may choose to pursue this later to see how our algorithms fair on this problem.  Other smaller features such as a graphic visualization of the solution would have also been nice.

If we were to redo this project from scratch, we would perhaps add some more abstraction between the algorithms, problems, and the solution representations and enforce these abstractions in order to allow the design of these components to be much more independent. 

Overall, this project was a great experience for all of us to work collaboratively in a relatively large group on a single project (versus working alone or with another partner for the p-sets). It was also a good experience having to designing all the abstractions and interfaces from scratch rather than just implementing given functions in a p-set.

## Advice for Future Students

Do not hesitate to tackle something that seems really hard!  Even though it may be a challenge, it is very rewarding to watch your work solve the problems of the world.


## References
1. Xin-She Yang; Deb, S., "Cuckoo Search via Lévy flights," Nature & Biologically Inspired Computing, 2009. NaBIC 2009. World Congress on , vol., no., pp.210,214, 9-11 Dec. 2009
doi: 10.1109/NABIC.2009.5393690
2. Ghodrati, Amirhossein, and Shahriar Lotfi. "A hybrid CS/PSO algorithm for global optimization." Intelligent Information and Database Systems. Springer Berlin Heidelberg, 2012. 89-98.
3. Trelea, Ioan Cristian. "The particle swarm optimization algorithm: convergence analysis and parameter selection." Information processing letters 85.6 (2003): 317-325.
4. Bratton, Daniel, and James Kennedy. "Defining a standard for particle swarm optimization." Swarm Intelligence Symposium, 2007. SIS 2007. IEEE. IEEE, 2007.
5. Molga, Marcin, and Czesław Smutnicki. "Test functions for optimization needs." Test functions for optimization needs (2005).

{% extends "writeup/base.html" %}
{% block writeup %}
<h1 id="thebirdsandthebees">The Birds and the Bees</h1>

<h4 id="samkimnickmerrillcrystalstowell">Sam Kim, Nick Merrill, Crystal Stowell</h4>

<div class='video'>
<iframe width="672" height="378" src="http://www.youtube.com/embed/0ohAn30qqAU?&vq=hd720" frameborder="0" allowfullscreen></iframe>
</div>

<h2 id="overview">Overview</h2>

<p>We built an infrastructure for optimization problems with the right amount of abstraction that would allow us to easily build new algorithms and problems independently. This way, we can swap in new algorithms or new problems without starting from scratch. Our main goal was to solve constraint programming problems, particularly the Nurse Scheduling Problem. We implemented 3 metaheuristic optimization algorithms: Cuckoo Search (CS), Particle Swarm Optimization (PSO), and a hybrid of the two algorithms. These algorithms can of course also solve more basic problems. We included four mathematical optimization test functions and three simple real-world problems in addition to the nurse scheduling problem to demonstrate the flexibility of our design as well as analyze the performance of the 3 algorithms.</p>

<p>The first algorithm, called the Cuckoo Search, is a metaheuristic optimization algorithm based on the behavior of the cuckoo bird [1] . Some cuckoo bird species parasitically lay their eggs in the nest of another bird species, where the likelihood of the host bird raising the bird as its own depends on how well the cuckoo mimics the egg of the host species. In this algorithm, an initial set of nests, which represent the solutions, are randomly generated. These solutions are then updated over multiple generations. The process of updating an individual solution is as follows: a random nest is chosen, and a new solution is generated by random-walking from this previous solution. This new solution can then replace a different randomly chosen solution if it has a fitness value better than the original. After this possible replacement of a solution, all of the nests are ranked by fitness and the worst fraction of the nests is replaced with random solutions. This combination of mechanisms allows the solutions to search locally and globally at the same time for the optimal solution.</p>

<p>The second algorithm, called the Particle Swarm Optimization, is a relatively popular algorithm introduced in 1995, and based on the social behavior of a swarm. Each particle, which represents a solution, has a velocity that allows the particle to move around the search space. Over each generation, the particle’s position is updated using its velocity, which is biased towards the particle’s own best position and the globally best position. This mechanism of swarm intelligence is expected to move the entire swarm towards the best solutions.</p>

<p>The last algorithm is a hybrid of the above two algorithms that combines all the above mechanisms in each iteration. First, a new solution is generated using the random walk which may replace another solution. Next, every solution is updated using the associated velocities as described above. Finally, the worst fraction of nests is replaced with random solutions. Although this has been proposed and implemented in literature, testing and analysis of its performance has been inadequate [2].</p>

<p>In addition, we built a website as the graphical user interface for our project. Originally a terminal UI, it was agreed upon that a website was both more interactive and more descriptive. <a href="http://optimizer.nickmerrill.me">Here</a> you can select a problem, read all about it, enter your desired inputs, and choose an algorithm with which to solve.</p>

<h2 id="planning">Planning</h2>

<p>Three weeks ago, we set out to write an optimization algorithm. The end goal was to be able to handle real-world problems as complex as the nurse scheduling problem and still find an optimal solution. That goal has been met. <strong>WOOT!</strong></p>

<p>To see how our project played out in terms of planning, check out our documentation for the <a href="{{STATIC_URL}}doc/draft_specification.pdf">initial draft</a> and the <a href="{{STATIC_URL}}doc/checkpoint2.pdf">final draft</a> of our technical specification.</p>

<p>In previous specifications, core features included optimization of an objective function using Cuckoo Search, providing a set of optimized solutions, and appropriate interfacing of the problem in order to change which optimization algorithm is used.</p>

<p>Six objective functions have been included: the Fence Problem, the Michalewicz Problem, the Box Problem, the Egg Holder Function, the Rastrigin Function, and the Nurse Scheduling Problem. The first two problems are univariate, the next two are bivariate, and the last two are multivariate.</p>

<p>Cool extensions that have also been implemented include additional optimization algorithms. In addition to the Cuckoo Search, there is also a Particle Swarm Optimization (PSO) algorithm as well as a hybrid of the two. A friendly graphical user interface has also been added as a cool extension to allow for better user interactivity and ease of use. This can be found at <a href="http://optimizer.nickmerrill.me">http://optimizer.nickmerrill.me</a>.</p>

<h2 id="designandimplementation">Design and Implementation</h2>

<p>Code: (<a href="https://github.com/NicholasMerrill/Optimizer">Backend</a> | <a href="https://github.com/NicholasMerrill/OptimizerFrontend">Frontend</a>)</p>

<p>Throughout the development of the project, the original design of the algorithms and solutions classes was altered to meet the needs of each class. Specifically, changes to the abstraction of the classes were made. However, this did not create any difficulties.</p>

<p>Since there are numerous parameters in each of the algorithms, parameter selection can significantly effect the algorithms&#8217; performance. For Cuckoo Search, we decided to follow many parameters used by the original author since these seemed to have the best results, confirmed by consequent studies [1]. This includes having 15 nests and setting p_a=0.25 (the percentage abandoned in each generation). There is a lot of ongoing research in the parameter selection for PSO, and for which situations certain parameters are appropriate. We decided to go with having 50 particles per generation, and setting w=0.7 (the weight particle&#8217;s momentum, influenced by its previous velocity), and phi_p,phi_g=1.5, both of which are weights given to velocity influenced by the local and global best solutions [3][4].</p>

<p>The separation and abstraction of the problems versus the algorithms made it very easy to add new problems. The abstract OptimizationProblem takes care of a lot of details related to variable constraints and interfacing with the algorithms and front-end. Implementing sub-classes only involves specifying these constraints and the fitness functions for the algorithm, and print functions for the front-end. For example, implementing the new test functions such as RastriginMinProb or EggholderFuncProb only took at most 10 minutes each to write.</p>

<p>Unfortunately, we did not get to implement fully everything we wanted. For example, although Cuckoo Search works well on all of the problems, PSO and the hybrid do not always reach the correct answer on problems with multivariable constraints (i.e. ax + by &lt; c). While they do reach an answer, it is not accurate enough to say that these work without bugs.</p>

<h3 id="analysis">Analysis</h3>

<p><strong>Experimental Procedures</strong>
Three mathematical functions commonly used in literature as benchmarks for optimization algorithms were chosen in order to analyze the performance of our three algorithms, including convergence rate and precision [5]. We used the Rastrigin Function, the Rosenbrock Function, and the Eggholder Function. Tests were run by first normalizing the number of iterations in each algorithm such that each algorithm would take approximately the same time for a given function, and then collecting data of the best fitness over the iterations. 200 data points were collected for each run, and 50 trials for each algorithm were averaged and analyzed.</p>

<p><strong>Rastrigin Function:</strong> This function is a fairly difficult problem due to its large search space and its large number of local minima which are regularly distributed. The plot to the right shows the function when n=2. Data was collected for n=10 over 3 seconds. As we can see in the graph below, the hybrid algorithm is very quick to converge to a good solution, but over a longer period of time the cuckoo search is able to find a better solution. The PSO performed the worst. The function is given below:</p>

<p>f(x) = An + ∑_(i=1)^n▒[ x_i^2-A cos(2πx_i ) where A=10 , x_i ∈ [&#8211;5.12,5.12]</p>

<p>Global minimum at x = 0,f(x) = 0</p>

<div class="images2">
    <img src="{{STATIC_URL}}img/rastrigin.jpg" />
    <img src="{{STATIC_URL}}img/rastgraph.png" />
</div>

<div class="images1">
    <img src="{{STATIC_URL}}img/rasttable.png" />
</div>

<p><strong>Rosenbrock’s Valley:</strong> This function is a classic optimization problem, also known as the banana function or the second function of De Jong. We can see in the plot for the function in 2 dimensions that the global minimum lies inside a long, narrow, flat valley. Although finding the valley is trivial, convergence to the global optimum is very difficult to the large search space. In order to gather meaningful data, we restricted the number of dimensions (n) to 2, and the search space to [&#8211;104, 104]. The function is defined as below:</p>

<p>f(x)=∑_(i=1)^(N&#8211;1)▒〖[(1-x_i )^2+100(x_(i+1)-〖x_i〗^2 )^2 ] ∀x∈R^N 〗</p>

<div class="images2">
    <img src="{{STATIC_URL}}img/rosenbrock.png" />
    <img src="{{STATIC_URL}}img/rosengraph.png" />
</div>

<div class="images1">
    <img src="{{STATIC_URL}}img/rosentable.png" />
</div>

<p>Note that the above data is plotted on a logarithmic scale, so CS performs very poorly in comparison to the other 2 algorithms, and levels off well before reaching the minimum value. This can likely be attributed to the random nature of CS’s local and global search which does not allow it to easily find better solutions inside the valley. PSO and the hybrid algorithm performed very similarly, with the PSO finding a slightly better solution towards the end. This further supports the conjecture that the random solutions and random walk do not contribute to finding the global minimum and that the swarm nature of the two algorithms allow them to move towards a better local minimum.</p>

<p><strong>Eggholder Function:</strong> This test function also contains numerous local minima, although both algorithms fared fairly well. Because both algorithms converged so quickly, data was only collected over half a second, which may place a greater emphasis on the overhead of setting up the initial population. We can see in the plot below that PSO and the hybrid converge very quickly to the global minimum. While CS takes slightly longer, it does reach the global minimum as well over a greater number of iterations (not shown in the graph).</p>

<p>f(x,y)= -(y+47)sin⁡(√(|y+x/2+47| ))-x sin⁡(√(|x-(y+47)| ))</p>

<p>Minimum at (x,y) = (512,404.2319)= &#8211;959.6407 for &#8211;512 ≤ x,y ≤ 512</p>

<div class="images2">
    <img src="{{STATIC_URL}}img/eggholder.jpg" alt="Eggholder Function" />
    <img src="{{STATIC_URL}}img/egggraph.png" alt="Eggholder performance table" />
</div>

<div class="images1">
    <img src="{{STATIC_URL}}img/eggtable.png" alt="Eggholder performance graph" />
</div>

<h2 id="reflection">Reflection</h2>

<p>We are very happy with the success of our project. </p>

<p>Throughout the past few weeks, there was some concern that the nurse scheduling problem might not be very generalizable. But after lots of reworking of the problem, it can be customized in a myriad of ways, allowing it to be useful for more than just a hospital. Additionally, we had trouble making this problem compatible with our algorithms due to the discrete nature of the solutions for the problem. This was resolved by still using continuous variables, but rounding when determining the fitness and constraints.</p>

<p>Had there been more time, we would have liked to add another complicated, real-world problem. For example, the Travelling Salesman Problem is a very well-known and interesting problem, so we may choose to pursue this later to see how our algorithms fair on this problem. Other smaller features such as a graphic visualization of the solution would have also been nice.</p>

<p><strong>If we were to redo this project from scratch, we would perhaps add some more abstraction between the algorithms, problems, and the solution representations and enforce these abstractions in order to allow the design of these components to be much more independent. Although it would likely be at the cost of computational speed (on top of the abstraction we already have), we had run into problems at one point of side-effects in the solutions class interfering with the algorithm design. Additionally, not only would this allow for much quicker implementation of algorithms to compare against even more algorithms, but it would reduce the issue we have right now of certain algorithms (PSO and hybrid) not completely working with certain problems.</strong></p>

<h2 id="adviceforfuturestudents">Advice for Future Students</h2>

<p>Do not hesitate to tackle something that seems really hard! Even though it may be a challenge, it is very rewarding to watch your work solve the problems of the world.</p>

<p><strong>OTHER</strong></p>

<h2 id="references">References</h2>

<ol>
<li>Xin-She Yang; Deb, S., &#8220;Cuckoo Search via Lévy flights,&#8221; Nature &amp; Biologically Inspired Computing, 2009. NaBIC 2009. World Congress on , vol., no., pp.210,214, 9&#8211;11 Dec. 2009
doi: 10.1109/NABIC.2009.5393690</li>
<li>Ghodrati, Amirhossein, and Shahriar Lotfi. &#8220;A hybrid CS/PSO algorithm for global optimization.&#8221; Intelligent Information and Database Systems. Springer Berlin Heidelberg, 2012. 89&#8211;98.</li>
<li>Trelea, Ioan Cristian. &#8220;The particle swarm optimization algorithm: convergence analysis and parameter selection.&#8221; Information processing letters 85.6 (2003): 317&#8211;325.</li>
<li>Bratton, Daniel, and James Kennedy. &#8220;Defining a standard for particle swarm optimization.&#8221; Swarm Intelligence Symposium, 2007. SIS 2007. IEEE. IEEE, 2007.</li>
<li>Molga, Marcin, and Czesław Smutnicki. &#8220;Test functions for optimization needs.&#8221; Test functions for optimization needs (2005).</li>
</ol>
{% endblock %}
